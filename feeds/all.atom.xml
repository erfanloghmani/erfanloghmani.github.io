<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Erfan Loghmani - Homepage</title><link href="http://ce.sharif.edu/~loghmani/" rel="alternate"></link><link href="http://ce.sharif.edu/~loghmani/feeds/all.atom.xml" rel="self"></link><id>http://ce.sharif.edu/~loghmani/</id><updated>2020-11-02T23:12:00+03:30</updated><entry><title>Being Frequentist? A Friendship Case</title><link href="http://ce.sharif.edu/~loghmani/being-frequentist.html" rel="alternate"></link><published>2020-11-02T23:12:00+03:30</published><updated>2020-11-02T23:12:00+03:30</updated><author><name>Erfan Loghmani</name></author><id>tag:ce.sharif.edu,2020-11-02:/~loghmani/being-frequentist.html</id><summary type="html">&lt;p&gt;You text a friend that you haven't seen for a long time. She/He replies, but not in a friendly manner, maybe just replying short answers like "I'm fine," "It's OK,"... After a week, you try again with the hope that you could schedule a phone/video call. She/He â€¦&lt;/p&gt;</summary><content type="html">&lt;p&gt;You text a friend that you haven't seen for a long time. She/He replies, but not in a friendly manner, maybe just replying short answers like "I'm fine," "It's OK,"... After a week, you try again with the hope that you could schedule a phone/video call. She/He says it's a good idea and will inform you whenever she/he had time. Days pass, and nothing happens. I've recently been in these situations a few times, and maybe sometimes I was on the other side of the table. The question that comes up is how much you should continue this process, and when you should lose your hope. Or on the other side, how much should you expect your friends to give you time.&lt;/p&gt;
&lt;p&gt;You may have been in these situations as well and may have solutions and techniques for yourself. When I was thinking about this problem weeks ago, I reminded two statistical inference approaches and tried to formalize the problem with that insight. Let's first take a look at the concepts.&lt;/p&gt;
&lt;h2&gt;Frequentist vs Bayesian&lt;/h2&gt;
&lt;p&gt;There are two main approaches in statistical inference, frequentist and bayesian. I will consider the parameter estimation task in this article and compare these two approaches in this task. The parameter estimation task rises in situations like estimating the average height of men in society or estimating the probability of getting heads when flipping a strange coin.&lt;/p&gt;
&lt;p&gt;The frequentist approach only uses data and happenings of the past for its estimation. For instance, if the frequentist approach wants to estimate the probability of heads for a coin, it will use the previous results and counts the number of heads divided by the total number of trials. On the other hand, the Bayesian approach starts with an initial belief and updates it based on observations. To better explain these two, we will see the mathematics behind them for the coin flip problem.&lt;/p&gt;
&lt;h2&gt;Maths for coin flip problem&lt;/h2&gt;
&lt;h3&gt;Frequentist's view&lt;/h3&gt;
&lt;p&gt;Consider we have a coin and have flipped it three times. The first two outcomes were heads, and the last one was tails. The frequentist approach starts without anything in mind because we don't have any prior experiences with the coin. After the first toss, as the outcome is heads, it estimates the probability to be \( \frac{1}{1} \). By the next two tosses, it will update the estimates to \( \frac{2}{2} \) and \( \frac{2}{3} \) consequently.&lt;/p&gt;
&lt;h3&gt;Bayesian's view&lt;/h3&gt;
&lt;p&gt;Analyzing the Bayesian approach is not that straightforward. First, we have to set the initial belief. For simplicity, I will consider the initial belief to have equal weights for these five potential values: \( \{0, \frac{1}{4}, \frac{2}{4}, \frac{3}{4}, 1\} \). We will consider the continuous initial belief in advance. The initial belief will be like:&lt;/p&gt;
&lt;p&gt;&lt;img alt="Initial belief" src="http://ce.sharif.edu/~loghmani/images/being-freq/dist-1.png"&gt;&lt;/p&gt;
&lt;p&gt;The Bayesian approach uses the Bayes formula to update its beliefs after each observation. That is&lt;/p&gt;
&lt;p&gt;&lt;img alt="Bayes Theorem" src="http://ce.sharif.edu/~loghmani/images/being-freq/1-bayes.png"&gt;&lt;/p&gt;
&lt;p&gt;In this example, \( \theta \) is any of the five potential probability values, and \( D \) is the observations. Let's compute the denominator for the first trial&lt;/p&gt;
&lt;p&gt;&lt;img alt="Probability of getting heads on first trial" src="http://ce.sharif.edu/~loghmani/images/being-freq/2-d1-head.png"&gt;&lt;/p&gt;
&lt;p&gt;This means before the first toss; we expect the outcome's probability of being heads to be \( \frac{1}{2} \).
Now let's compute the new probabilities of parameters after the first toss.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Updated probabilities after first trial" src="http://ce.sharif.edu/~loghmani/images/being-freq/3-update-after-d1.png"&gt;&lt;/p&gt;
&lt;p&gt;We see that after the first observation, the Bayesian approach stops putting any weight on the \( \theta = 0 \) condition, as it has seen one head so far. But unlike the frequentist approach, it doesn't immediately says that the probability of getting heads is one and still puts some weights on other possibilities. The calculation continues as&lt;/p&gt;
&lt;p&gt;&lt;img alt="Probability of getting heads on second trial" src="http://ce.sharif.edu/~loghmani/images/being-freq/4-d2-head.png"&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="Updated probabilities after second trial" src="http://ce.sharif.edu/~loghmani/images/being-freq/5-update-after-d2.png"&gt;&lt;/p&gt;
&lt;p&gt;And finally&lt;/p&gt;
&lt;p&gt;&lt;img alt="Probability of getting tails on third trial" src="http://ce.sharif.edu/~loghmani/images/being-freq/6-d3-tail.png"&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="Updated probabilities after third trial" src="http://ce.sharif.edu/~loghmani/images/being-freq/7-update-after-d3.png"&gt;&lt;/p&gt;
&lt;p&gt;The Bayesian approach started with a uniform belief and updated it to give more weights on probabilities near one after seeing the first two heads. Following the third observation, it also removes the weight from \( \theta = 1 \) and skews the probabilities toward lower values.&lt;/p&gt;
&lt;h3&gt;Comparison&lt;/h3&gt;
&lt;p&gt;The frequentist approach gives a unique value for the estimated probability, while the Bayesian approach holds a probability distribution over the values. To reach one estimation value for the Bayesian method, we could take the expectation of the distribution. This way, the estimated probability of getting head in these two approaches will be:&lt;/p&gt;
&lt;p&gt;&lt;img alt="Estimated probabilities for each approach" src="http://ce.sharif.edu/~loghmani/images/being-freq/8-table.png"&gt;&lt;/p&gt;
&lt;h3&gt;Continuous version&lt;/h3&gt;
&lt;p&gt;While I had simplified the above example to a discrete version, you can play with the continuous version of the bayesian approach starting from uniform distribution in the section below.&lt;/p&gt;
&lt;iframe id="prior" width="100%" height="480" src="bayesian-inference/index.html"&gt;&lt;/iframe&gt;

&lt;p&gt;I have taken the codes for this part from the &lt;a href="https://seeing-theory.brown.edu/bayesian-inference/index.html"&gt;Seeing Theory&lt;/a&gt; website. You can check it out and find other exciting visualizations for probability and statistics concepts.&lt;/p&gt;
&lt;p&gt;In the continuous version, &lt;a href="http://www.cs.tau.ac.il/~mansour/ml-course-12/Scribe2_Bayes.pdf"&gt;it is shown that&lt;/a&gt; the expectation of the Bayesian estimate is \( \frac{H + 1}{N + 2} \), where \( H \) is the number of heads and \( N \) is the total number of trials. On the other hand, the frequentist approach estimates the parameter to be \( \frac{H}{N} \). It is somehow like that the Bayesian method adds one head and one tail to the beginning of the sequence.&lt;/p&gt;
&lt;h2&gt;Back to the friendship case&lt;/h2&gt;
&lt;p&gt;The friendship scenarios are related to the coin flip example as I always have to estimate my friend's probability of responding appropriately. Then, maybe if I see the estimated probability is less than a threshold, I will lose my hope and don't text her/him the next time.&lt;/p&gt;
&lt;p&gt;Let's define an optimistic line that indicates how hopeful you are about your friend's next behavior.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Optimistic line" src="http://ce.sharif.edu/~loghmani/images/being-freq/9-opt-line.png"&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style="font-size:0.9em;"&gt;&lt;strong&gt;Notice:&lt;/strong&gt; The faces are designed by Bonita90 from &lt;a href="https://99designs.co.uk/buttons-icons/contests/button-icon-cancerlife-105934"&gt;99design&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;We want to see where each statistical approach will stand on this line in different situations.&lt;/p&gt;
&lt;p&gt;First, consider a situation where you had two unsuccessful attempts in connecting with your friend. The frequentist approach estimates the probability as \( \frac{0}{2} \) while the Bayesian approach's average estimate will be \( \frac{1}{4} \). So the line will be&lt;/p&gt;
&lt;p&gt;&lt;img alt="Optimistic line after two bad happenings" src="http://ce.sharif.edu/~loghmani/images/being-freq/10-opt-line-bad.png"&gt;&lt;/p&gt;
&lt;p&gt;Oppositely, consider a situation where both of your first attempts were successful. Now the frequentist approach will estimate \( \frac{2}{2} \), and the Bayesian approach will estimate \( \frac{3}{4} \) on average. And we get&lt;/p&gt;
&lt;p&gt;&lt;img alt="Optimistic line after two good happenings" src="http://ce.sharif.edu/~loghmani/images/being-freq/11-opt-line-good.png"&gt;&lt;/p&gt;
&lt;p&gt;What about you? Where do you stand on the optimistic line in each of these scenarios? Of course, the answer may differ based on other factors, the details of what has happened between you, or maybe your personality and how much you like that friend. When I discussed this issue with some of my friends, some of them told me they are more optimistic in most cases because they know that everyone could face hard times and we have to be more patient in friendships.&lt;/p&gt;
&lt;p&gt;While you are free to decide where you want to stand, personally, I think the frequentist approach is not a good option. I have some problems with this approach in both examples. It seems to me that the frequentist approach makes a harsh decision too early that may have harmful consequences. In the first example, it may lose hope early, causing a valuable friendship to die. On the other side, being optimistic based on a few good experiences may higher the expectations, and future bad outcomes may hurt you more. Overall, I think taking a moment before each decision and seeing it from this perspective is helpful. We could check how much evidence we have for our decisions and how confident we should be based on them.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Let me know your thoughts on this issue. Also, it will be great if you mention what other things we should consider in our modeling.&lt;/strong&gt;&lt;/p&gt;
&lt;h2&gt;Thanks&lt;/h2&gt;
&lt;p&gt;Thank you, Matin Ansaripour, Bahar Salamatian, and Atia Hamidizadeh, for providing your time to discuss this topic, which helped me see the subject from new perspectives and organize this blog post. Additionally, Sina Rismanchian, Tannaz Azari, and again Bahar Salamatian to proofread the article and providing their opinions and excellent suggestions.&lt;/p&gt;</content><category term="Life"></category><category term="Blog"></category></entry><entry><title>M.Sc. seminar</title><link href="http://ce.sharif.edu/~loghmani/msc-seminar.html" rel="alternate"></link><published>2020-01-30T20:34:00+03:30</published><updated>2020-01-30T20:34:00+03:30</updated><author><name>Erfan Loghmani</name></author><id>tag:ce.sharif.edu,2020-01-30:/~loghmani/msc-seminar.html</id><summary type="html">&lt;h1&gt;Representation Learning on Dynamic Graphs&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;Abstract:&lt;/strong&gt; Graphs are a common language in modeling several problems, from social and economic networks to interactions in cells and brain neurons.
According to the availability of an enormous amount of data from graphs, Machine Learning algorithms gained lots of attention in this area. But â€¦&lt;/p&gt;</summary><content type="html">&lt;h1&gt;Representation Learning on Dynamic Graphs&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;Abstract:&lt;/strong&gt; Graphs are a common language in modeling several problems, from social and economic networks to interactions in cells and brain neurons.
According to the availability of an enormous amount of data from graphs, Machine Learning algorithms gained lots of attention in this area. But the main challenge is how to represent and encode nodes so that such models could interpret the data and the representation preserve the main features of nodes.&lt;/p&gt;
&lt;p&gt;There are different types of methods for this problem, some are based on a distance function defined on nodes, some are based on random walks like DeepWalk and node2vec, there is also a range of methods based on Deep Learning models like Graph Convolutional Networks and GraphSAGE.
These methods are originally designed for static graphs and learn representations for one snapshot of a graph, while in most problems graphs evolve in time and new nodes and edges arrive and leave and the model has to encounter these events and update representations meanwhile.&lt;/p&gt;
&lt;p&gt;In this research, we are going to first examine the latest methods proposed for dynamic graph representation learning and then try to enhance them in tasks like link prediction and node classification.&lt;/p&gt;
&lt;p&gt;&lt;a href="http://ce.sharif.edu/~loghmani/files/MSc-seminar-presentation.pdf" target="_blank"&gt;Presentation file&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;embed src="https://drive.google.com/viewerng/
viewer?embedded=true&amp;url=http://ce.sharif.edu/~loghmani/files/MSc-seminar-presentation.pdf" width="100%" height="450"&gt;&lt;/p&gt;</content><category term="Presentation"></category><category term="Academia"></category></entry><entry><title>ICTP workshop</title><link href="http://ce.sharif.edu/~loghmani/ictp-workshop.html" rel="alternate"></link><published>2019-10-12T20:21:00+03:30</published><updated>2019-10-12T20:21:00+03:30</updated><author><name>Erfan Loghmani</name></author><id>tag:ce.sharif.edu,2019-10-12:/~loghmani/ictp-workshop.html</id><summary type="html">&lt;p&gt;Last week I had the chance to attend the &lt;a href="http://indico.ictp.it/event/8722/overview"&gt;ICTP workshop on science of data science&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;From the very beginning of landing at Venice to the last moments of my visit, there was lots of learning for me. Not just from the lectures, but from all the people I've met â€¦&lt;/p&gt;</summary><content type="html">&lt;p&gt;Last week I had the chance to attend the &lt;a href="http://indico.ictp.it/event/8722/overview"&gt;ICTP workshop on science of data science&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;From the very beginning of landing at Venice to the last moments of my visit, there was lots of learning for me. Not just from the lectures, but from all the people I've met, and maybe some from myself :)
Thank you, organizers, to bring all these people together and also for your kindness &amp;amp; hospitality.&lt;/p&gt;
&lt;p&gt;&lt;img alt="ICTP workshop on science of data science group photo" src="http://indico.ictp.it/event/8722/material/2/0.jpg"&gt;&lt;/p&gt;
&lt;p&gt;I also had a great experience presenting an intoductory poster of my research problem and ongoing works in the poster session.&lt;/p&gt;
&lt;p&gt;&lt;a href="http://ce.sharif.edu/~loghmani/files/ICTP_Poster.pdf" target="_blank"&gt;My poster&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;embed src="https://drive.google.com/viewerng/
viewer?embedded=true&amp;url=http://ce.sharif.edu/~loghmani/files/ICTP_Poster.pdf" width="100%" height="800"&gt;&lt;/p&gt;</content><category term="Poster"></category><category term="Academia"></category><category term="Travel"></category></entry><entry><title>Don't Repeat Yourself</title><link href="http://ce.sharif.edu/~loghmani/dont-repeat-yourself.html" rel="alternate"></link><published>2019-09-06T18:17:00+04:30</published><updated>2019-09-06T18:17:00+04:30</updated><author><name>Erfan Loghmani</name></author><id>tag:ce.sharif.edu,2019-09-06:/~loghmani/dont-repeat-yourself.html</id><summary type="html">&lt;p&gt;You may have heard about the DRY principle in coding &amp;amp; software design. It says that by using function definitions and OOP concepts you can avoid repeating the code. But I'm not going to write about the DRY principle this way; I found it somewhere else in my life.&lt;/p&gt;
&lt;p&gt;Last week â€¦&lt;/p&gt;</summary><content type="html">&lt;p&gt;You may have heard about the DRY principle in coding &amp;amp; software design. It says that by using function definitions and OOP concepts you can avoid repeating the code. But I'm not going to write about the DRY principle this way; I found it somewhere else in my life.&lt;/p&gt;
&lt;p&gt;Last week I was a mentor in &lt;a href="http://summerschool.ce.sharif.edu/register"&gt;CSSS2019&lt;/a&gt; which stands for Computer Science Summer School 2019. The school was aimed to gather high school students and show them some inspiring and insightful topics in CS. In the two weeks of the school, I facilitated the same group discussion six times, but to six different groups of students. I was one of the mentors of the probability workshop and we wanted to start the class with a group discussion about the meaning of probability. After the fourth class, I suddenly saw a pattern. The discussions in the first class of each day went far better than in the second class. So I tried to find the reason and here's what I found: When the first class went well, I just wanted to repeat them in the second class. So the examples were not original and didn't work, I didn't respond well to students and make them participate. And the most important thing I found is I forgot to love the students, not the topic. With the experience of the first class of the day and the good feedback I got, maybe my mind increased the value of the topic and caused me to forget loving the students. After seeing these facts, I think the two remaining classes went extremely better and I had a better feeling at the end of both classes.&lt;/p&gt;
&lt;p&gt;This situation happens frequently, when we do something well once, we try to repeat that in the same way next time, but it doesn't work like that. For example, if once I felt sad and after going out with friends I felt better, the next time that I'm sad if I just try to repeat that and schedule a hangout it may not work. Because this time I missed the original goal of hangouts to see friends and I'm just using it to solve my problem.&lt;/p&gt;
&lt;p&gt;I'm trying this myself, and hope you can too.&lt;/p&gt;
&lt;p&gt;&lt;img alt="CSSS 2019 Tehran" src="http://ce.sharif.edu/~loghmani/images/csss2019-all.jpg"&gt;&lt;/p&gt;</content><category term="Blog"></category><category term="Life"></category></entry><entry><title>Start</title><link href="http://ce.sharif.edu/~loghmani/start.html" rel="alternate"></link><published>2019-06-29T23:43:00+04:30</published><updated>2019-06-29T23:43:00+04:30</updated><author><name>Erfan Loghmani</name></author><id>tag:ce.sharif.edu,2019-06-29:/~loghmani/start.html</id><summary type="html">&lt;p&gt;Everything has an start!&lt;/p&gt;</summary><content type="html">&lt;p&gt;Everything has an start!&lt;/p&gt;</content><category term="Blog"></category></entry><entry><title>My B.Sc. project</title><link href="http://ce.sharif.edu/~loghmani/bsc-project.html" rel="alternate"></link><published>2018-07-03T18:17:00+04:30</published><updated>2018-07-03T18:17:00+04:30</updated><author><name>Erfan Loghmani</name></author><id>tag:ce.sharif.edu,2018-07-03:/~loghmani/bsc-project.html</id><summary type="html">&lt;h1&gt;Profiling Researchers Based on Features Extracted from Articles and Citations&lt;/h1&gt;
&lt;p&gt;A researcher may be evaluated based on several measures such as the total number of published papers, number of citations,  and her/his h-index. These measures may not reflect the real quality of researchers as many self citations and citing â€¦&lt;/p&gt;</summary><content type="html">&lt;h1&gt;Profiling Researchers Based on Features Extracted from Articles and Citations&lt;/h1&gt;
&lt;p&gt;A researcher may be evaluated based on several measures such as the total number of published papers, number of citations,  and her/his h-index. These measures may not reflect the real quality of researchers as many self citations and citing co-authors are observed during last decades. On the other hand, having a realistic quantitative measure for scientists has enormous impact on decisions taken in academia including hiring and  promoting faculty members. This research aims to develop a mathematical model for evaluating scientific researchers. The model uses authors' interactions and provides a quantitative value as a proxy of authorâ€™s quality. There are two existing models named R-model and Q-model for the same purpose. Our proposed model improves on these models by incorporating an iterative strategy to obtain best quantitate evolution of the researchers. Practically, we have tested our model  on DBLP dataset.&lt;/p&gt;
&lt;p&gt;I've done my B.Sc. project under &lt;a href="http://sharif.edu/~motahari/index.html"&gt;Dr. Motahari's&lt;/a&gt; supervision.&lt;/p&gt;
&lt;p&gt;&lt;a href="http://ce.sharif.edu/~loghmani/files/profiling-researchers.pdf" target="_blank"&gt;Presentation file&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;embed src="https://drive.google.com/viewerng/
viewer?embedded=true&amp;url=http://ce.sharif.edu/~loghmani/files/profiling-researchers.pdf" width="100%" height="450"&gt;&lt;/p&gt;</content><category term="Presentation"></category><category term="Academia"></category></entry></feed>