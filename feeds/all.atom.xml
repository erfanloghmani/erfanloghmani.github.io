<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Erfan Loghmani - Homepage</title><link href="https://erfanloghmani.github.io/" rel="alternate"></link><link href="https://erfanloghmani.github.io/feeds/all.atom.xml" rel="self"></link><id>https://erfanloghmani.github.io/</id><updated>2025-06-25T07:13:00-07:00</updated><entry><title>Aligning Language Models with Observational Data</title><link href="https://erfanloghmani.github.io/aligning-language-models-observational-data.html" rel="alternate"></link><published>2025-06-25T07:13:00-07:00</published><updated>2025-06-25T07:13:00-07:00</updated><author><name>Erfan Loghmani</name></author><id>tag:erfanloghmani.github.io,2025-06-25:/aligning-language-models-observational-data.html</id><summary type="html">&lt;p&gt;Can we align language models using observational data instead of costly experiments like A/B tests? In my latest research, titled "Aligning Language Models with Observational Data: Opportunities and Risks from a Causal Perspective," we show that the answer is yes, but you should be careful about causality.&lt;/p&gt;
&lt;p&gt;Language models …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Can we align language models using observational data instead of costly experiments like A/B tests? In my latest research, titled "Aligning Language Models with Observational Data: Opportunities and Risks from a Causal Perspective," we show that the answer is yes, but you should be careful about causality.&lt;/p&gt;
&lt;p&gt;Language models (LLMs) are increasingly used to generate content that drives business outcomes, such as click-through rates, engagement, and medical adherence. But these models aren't aligned with such goals out of the box. Fine-tuning with experimental data (e.g., A/B tests) helps, but it's costly and comes with significant engineering and logistical challenges. Meanwhile, most firms already have massive amounts of historical (observational) data. We find that historical data &lt;em&gt;can&lt;/em&gt; be used instead, but it requires careful attention to causality.&lt;/p&gt;
&lt;p&gt;Want to know more? Check out the project webpage at &lt;a href="https://deconfoundlm.github.io"&gt;deconfoundlm.github.io&lt;/a&gt; or read the paper on &lt;a href="https://arxiv.org/abs/2506.00152"&gt;arXiv&lt;/a&gt;.&lt;/p&gt;</content><category term="Blog"></category><category term="Academia"></category><category term="Blog"></category></entry><entry><title>Haring Symposium</title><link href="https://erfanloghmani.github.io/haring-symposium-discussion.html" rel="alternate"></link><published>2025-04-15T15:42:00-07:00</published><updated>2025-04-15T15:42:00-07:00</updated><author><name>Erfan Loghmani</name></author><id>tag:erfanloghmani.github.io,2025-04-15:/haring-symposium-discussion.html</id><summary type="html">&lt;p&gt;This past weekend, I had the wonderful opportunity to attend the &lt;a href="https://kelley.iu.edu/faculty-research/departments/marketing/events/haring-symposium/index.html"&gt;Haring Symposium&lt;/a&gt; at Marketing Department @ Kelley School of Business! It was my first time participating in an event specifically designed for the academic development of PhD students, and I absolutely loved it. A warm thank you to the organizers …&lt;/p&gt;</summary><content type="html">&lt;p&gt;This past weekend, I had the wonderful opportunity to attend the &lt;a href="https://kelley.iu.edu/faculty-research/departments/marketing/events/haring-symposium/index.html"&gt;Haring Symposium&lt;/a&gt; at Marketing Department @ Kelley School of Business! It was my first time participating in an event specifically designed for the academic development of PhD students, and I absolutely loved it. A warm thank you to the organizers, Sarang Sunder, Dr. Mansur Khamitov (PhD, MBA), Saang A Lee, Guangqi Dong, and everyone else who helped make it such a meaningful and well-organized event.&lt;/p&gt;
&lt;p&gt;I had the pleasure of discussing the paper "Review Manipulation and Platform Policy: Evidence from Online Travel Agencies" by Liujie Qian, Zhexi Zhang, and Hongju Liu. It is an excellent piece of work tackling the important and timely topic of online review manipulation. The authors approached the problem from a fresh perspective and contributed valuable insights to the field by leveraging a compelling natural experiment setting. I really enjoyed reading it and hope my comments and suggestions were helpful. Looking forward to seeing it published soon!&lt;/p&gt;
&lt;p&gt;&lt;img alt="Erfan Loghmani discussing the paper &amp;quot;Review Manipulation and Platform Policy: Evidence from Online Travel Agencies&amp;quot; by Liujie Qian, Zhexi Zhang, and Hongju Liu at the 2025 Haring Symposium." src="https://erfanloghmani.github.io/images/haring-symposium/erfan-loghmani-haring-discussion.jpeg"&gt;&lt;/p&gt;
&lt;p&gt;The symposium offered a unique space to connect with fellow PhD students, learn about their research, exchange ideas, and share experiences about our journeys in academia. The invited faculty members and guest lecturers were incredibly insightful. It was both encouraging and grounding to hear from scholars who have someday been a PhD student and are now contributing so meaningfully to the field.&lt;/p&gt;
&lt;p&gt;It was also a pleasure to attend my friend and cohort member &lt;a href="https://foster.uw.edu/academics/degree-programs/phd-program/directory/haonan-zhang/"&gt;Haonan Zhang&lt;/a&gt; present his fantastic joint work with Shirsho Biswas on "Does Online Shopping Affect Offline Price Sensitivity?" You nailed it!&lt;/p&gt;
&lt;p&gt;&lt;img alt="UW and Guangqi at Haring Symposium 2025" src="https://erfanloghmani.github.io/images/haring-symposium/haring-uw-and-guangqi.jpeg"&gt;&lt;/p&gt;
&lt;p&gt;Outside the academic sessions, the trip had so many other highlights for me. I had a great time chatting with fellow participants and getting to know them beyond just their research. The IU campus was absolutely beautiful, and I loved its lively vibe. Huge thanks as well to Guangqi Dong for his hospitality and kindness while hosting us.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Beautiful IU" src="https://erfanloghmani.github.io/images/haring-symposium/haring-beautiful-iu.jpeg"&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="Lunch" src="https://erfanloghmani.github.io/images/haring-symposium/haring-lunch.jpeg"&gt;&lt;/p&gt;</content><category term="Blog"></category><category term="Academia"></category><category term="Blog"></category><category term="Travel"></category></entry><entry><title>TOPS Presentation</title><link href="https://erfanloghmani.github.io/tops-presentation.html" rel="alternate"></link><published>2025-02-28T16:17:00-08:00</published><updated>2025-02-28T16:17:00-08:00</updated><author><name>Erfan Loghmani</name></author><id>tag:erfanloghmani.github.io,2025-02-28:/tops-presentation.html</id><summary type="html">&lt;h1&gt;TOPS Presentation&lt;/h1&gt;
&lt;p&gt;Last week I had the opportunity to present our work, "Investigating the Impact of Advertising on Smoking Cessation: The Role of Direct-to-Consumer Prescription Drug Advertising," co-authored with &lt;a href="https://faculty.washington.edu/agoli/"&gt;Ali Goli&lt;/a&gt;, at the &lt;a href="https://www.tobaccopolicy.org"&gt;Tobacco Online Policy Seminar (TOPS)&lt;/a&gt;. It was a great experience engaging with a multidisciplinary audience and discussing …&lt;/p&gt;</summary><content type="html">&lt;h1&gt;TOPS Presentation&lt;/h1&gt;
&lt;p&gt;Last week I had the opportunity to present our work, "Investigating the Impact of Advertising on Smoking Cessation: The Role of Direct-to-Consumer Prescription Drug Advertising," co-authored with &lt;a href="https://faculty.washington.edu/agoli/"&gt;Ali Goli&lt;/a&gt;, at the &lt;a href="https://www.tobaccopolicy.org"&gt;Tobacco Online Policy Seminar (TOPS)&lt;/a&gt;. It was a great experience engaging with a multidisciplinary audience and discussing our research methodology and findings.&lt;/p&gt;
&lt;p&gt;A big thank you to the TOPS organizers for curating such a valuable seminar series and facilitating such an engaging discussion! The session recording is now out and you find it on &lt;a href="https://www.youtube.com/watch?v=EFegV3aHTE8"&gt;YouTube&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Check out our &lt;a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4775370"&gt;paper&lt;/a&gt; for a detailed look at our findings and results. If you have any feedback or comments, feel free to email Ali or me, and we will be happy to chat.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://erfanloghmani.github.io/files/TOPS-Investigating the Impact of Advertising on Smoking Cessation.pdf" target="_blank"&gt;Presentation file&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;embed src="https://drive.google.com/viewerng/
viewer?embedded=true&amp;url=https://erfanloghmani.github.io/files/TOPS-Investigating the Impact of Advertising on Smoking Cessation.pdf" width="100%" height="450"&gt;&lt;/p&gt;</content><category term="Blog"></category><category term="Presentation"></category><category term="Academia"></category><category term="Blog"></category></entry><entry><title>From Frustration to Fast: Using Ray for Parallel Computing on a Single Machine or a Cluster</title><link href="https://erfanloghmani.github.io/from-frustration-to-fast-how-to-ray.html" rel="alternate"></link><published>2023-05-09T12:42:00-07:00</published><updated>2023-05-09T12:42:00-07:00</updated><author><name>Erfan Loghmani</name></author><id>tag:erfanloghmani.github.io,2023-05-09:/from-frustration-to-fast-how-to-ray.html</id><summary type="html">&lt;p&gt;If you're someone who works with data and runs computationally-intensive tasks, you know that multiprocessing can be a game changer. It can speed up your work significantly and save you precious time. I previously used the &lt;code&gt;multiprocessing&lt;/code&gt; Python package for running my jobs concurrently, but that didn’t always go …&lt;/p&gt;</summary><content type="html">&lt;p&gt;If you're someone who works with data and runs computationally-intensive tasks, you know that multiprocessing can be a game changer. It can speed up your work significantly and save you precious time. I previously used the &lt;code&gt;multiprocessing&lt;/code&gt; Python package for running my jobs concurrently, but that didn’t always go straightforward. I often had challenges when using multiprocessing in jupyter, and it also didn’t deal with errors well. Sometimes I experienced that the jobs would get stuck if the code raised an error without logging the error appropriately.&lt;/p&gt;
&lt;p&gt;&lt;img alt="A generated image of a person working with an old computer, Image credit: Microsoft/Bing" src="https://erfanloghmani.github.io/images/from-frustration-to-fast-how-to-ray/waiting-person.jpeg"&gt;&lt;/p&gt;
&lt;p&gt;That's where Ray comes in. &lt;a href="https://www.ray.io/"&gt;Ray&lt;/a&gt; is a Python library that has been a real lifesaver for me. It's not only lightning-fast and user-friendly, but it also allows you to distribute your tasks across multiple machines for even faster computation. Additionally, it offers built-in integrations for machine learning and data streaming.&lt;/p&gt;
&lt;p&gt;While there are good resources available online for using Ray on a single machine (like &lt;a href="https://rise.cs.berkeley.edu/blog/ray-tips-for-first-time-users/"&gt;this one&lt;/a&gt;), there's limited information on how to use it to distribute tasks across multiple machines and manually set up a cluster. As someone who has recently explored this feature of Ray, I want to share my experience with you in this blog post. If you're interested in speeding up your computations and making the most of Ray's capabilities, keep reading!&lt;/p&gt;
&lt;p&gt;I'll first walk you through a simple simulation of SGD (stochastic gradient descent) and how to parallelize it for faster computation. We'll start by parallelizing the simulation on a single machine using Ray, and then we'll take things up a notch by distributing the task across multiple machines.&lt;/p&gt;
&lt;h2&gt;SGD on OLS&lt;/h2&gt;
&lt;p&gt;In this section, I'll provide some background on the simple SGD job that we'll be parallelizing using Ray. Let's consider the ordinary least squares (OLS) setting, where we have n observations of covariates x and independent variables y, and they have a linear relationship in the form of \(y = x^T \beta + \epsilon\), where \( \beta \) is the unknown vector of coefficients and epsilon is the error term. Our goal is to obtain an estimate of \( \beta \) using the SGD algorithm. Of course, you can simply use the OLS close-formed solution. :)&lt;/p&gt;
&lt;p&gt;The SGD algorithm is an iterative optimization technique that randomly selects a data point and updates the estimates of the coefficients based on the error between the predicted and actual values. In each iteration, the algorithm updates the estimate of beta using the gradient of the loss function with respect to beta.&lt;/p&gt;
&lt;p&gt;For this problem, our data-generating process looks like this&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;n&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;10000&lt;/span&gt;
&lt;span class="n"&gt;d&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;100&lt;/span&gt;

&lt;span class="n"&gt;X&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;normal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="n"&gt;beta&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;normal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="n"&gt;eps&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;0.1&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;normal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="n"&gt;Y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;beta&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;eps&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Where X is the pooled matrix with each row containing one of the covariate observations, and y is the stacked version of the independent variable.&lt;/p&gt;
&lt;p&gt;The SGD method takes one of the data points at random and updates the estimate using the gradient of the estimation loss.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;idx&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;randint&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;x_step&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_step&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;idx&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;:]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reshape&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;Y&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;idx&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;grad_step&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;gradient_step&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x_step&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_step&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;beta_hat&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;beta_hat&lt;/span&gt; &lt;span class="o"&gt;-=&lt;/span&gt; &lt;span class="n"&gt;lr&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;grad_step&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The complete function looks like this, where I also store the series of step losses and a log of estimates and distances to the true parameter.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;gradient_step&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x_step&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_step&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;beta_hat&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
   &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;x_step&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y_step&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;x_step&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;beta_hat&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;


&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;sgd_loss&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;Y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;beta&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;lr&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.01&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
   &lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;d&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;
   &lt;span class="n"&gt;beta_hat&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;normal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
   &lt;span class="n"&gt;loss_array&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
   &lt;span class="n"&gt;dist_array&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
   &lt;span class="n"&gt;estimate_log_every&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;100&lt;/span&gt;
   &lt;span class="n"&gt;estimate_array&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt; &lt;span class="o"&gt;//&lt;/span&gt; &lt;span class="n"&gt;estimate_log_every&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
   &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;t&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
       &lt;span class="n"&gt;idx&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;randint&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
       &lt;span class="n"&gt;x_step&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_step&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;idx&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;:]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reshape&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;Y&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;idx&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
       &lt;span class="n"&gt;grad_step&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;gradient_step&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x_step&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_step&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;beta_hat&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
       &lt;span class="n"&gt;beta_hat&lt;/span&gt; &lt;span class="o"&gt;-=&lt;/span&gt; &lt;span class="n"&gt;lr&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;grad_step&lt;/span&gt;
       &lt;span class="n"&gt;step_loss&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y_step&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;x_step&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;beta_hat&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="o"&gt;**&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;
       &lt;span class="n"&gt;loss_array&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;t&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;step_loss&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
       &lt;span class="n"&gt;dist_array&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;t&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;linalg&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;norm&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;beta_hat&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;beta&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
       &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;t&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="n"&gt;estimate_log_every&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
           &lt;span class="n"&gt;estimate_array&lt;/span&gt;&lt;span class="p"&gt;[(&lt;/span&gt;&lt;span class="n"&gt;t&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;//&lt;/span&gt; &lt;span class="n"&gt;estimate_log_every&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;:]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;beta_hat&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reshape&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
   &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;loss_array&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dist_array&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;estimate_array&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Running this function for &lt;code&gt;T = 80000&lt;/code&gt; and 100 repetitions without multiprocessing takes 3 min and 19s for me.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;T&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;80000&lt;/span&gt;
&lt;span class="n"&gt;rounds&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;100&lt;/span&gt;
&lt;span class="n"&gt;estimate_log_every&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;100&lt;/span&gt;


&lt;span class="n"&gt;all_results_single&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
&lt;span class="n"&gt;start_time&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;time&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;time&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;r&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;trange&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;rounds&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
   &lt;span class="n"&gt;result&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;sgd_loss&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;Y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;beta&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;beta&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;lr&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;lr&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;estimate_log_every&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;estimate_log_every&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
   &lt;span class="n"&gt;all_results_single&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;result&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;duration&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;time&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;time&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;start_time&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Runs using single process took &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;duration&lt;/span&gt;&lt;span class="si"&gt;:&lt;/span&gt;&lt;span class="s2"&gt;.3f&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt; seconds&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h2&gt;Multiprocessing on a Single Machine&lt;/h2&gt;
&lt;p&gt;The process starts with installing ray. You could simply do&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;pip install ray
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;or &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;pip install -U &lt;span class="s2"&gt;&amp;quot;ray[default]&amp;quot;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;if you want to have the ray dashboard, a web application that shows how the runs are going, you can access the logs for each run.&lt;/p&gt;
&lt;p&gt;Then you only need to do these steps&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;import ray&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Add &lt;code&gt;@ray.remote&lt;/code&gt; annotator to your function. Let's say the function has the name &lt;code&gt;my_func&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Call &lt;code&gt;ray.init()&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Run the function concurrently using &lt;code&gt;ray.get([my_func.remote() for r in range(rounds)])&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In my code, because I wanted both single-process and multiprocess versions of the function, I created a new function for the remote case that just calls the SGD function.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="nd"&gt;@ray&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;remote&lt;/span&gt;
&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;sgd_loss_remote&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;Y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;beta&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;lr&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;estimate_log_every&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
   &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;sgd_loss&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;Y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;beta&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;lr&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;estimate_log_every&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Running the processes then looks like:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;ray&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;init&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;start_time&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;time&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;time&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;all_results_remote&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ray&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
   &lt;span class="p"&gt;[&lt;/span&gt;
       &lt;span class="n"&gt;sgd_loss_remote&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;remote&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
           &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;Y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;beta&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;beta&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;lr&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;lr&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;estimate_log_every&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;estimate_log_every&lt;/span&gt;
       &lt;span class="p"&gt;)&lt;/span&gt;
       &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;r&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;rounds&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
   &lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;duration&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;time&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;time&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;start_time&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Runs using multiple processes took &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;duration&lt;/span&gt;&lt;span class="si"&gt;:&lt;/span&gt;&lt;span class="s2"&gt;.3f&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt; seconds&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;This way, it only takes 16 seconds to finish! I got a significant win on time by using all of the CPU cores just after adding a few lines.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Result of htop command showing all of the CPU cores are used" src="https://erfanloghmani.github.io/images/from-frustration-to-fast-how-to-ray/htop-result.png"&gt;&lt;/p&gt;
&lt;h2&gt;Run a ray cluster manually&lt;/h2&gt;
&lt;p&gt;Ray offers &lt;a href="https://docs.ray.io/en/latest/cluster/getting-started.html"&gt;documentation&lt;/a&gt; for setting up Ray clusters on AWS or GCP servers, as well as utilizing Kubernetes. If these options align better with your requirements, I recommend referring to the documentation for detailed instructions. However, keep reading the post if you want to avoid the initial steps of setting up Kubernetes or wish to use Ray on your machines. In this section, I will focus on the scenario where you prefer to run a Ray cluster on multiple machines manually.&lt;/p&gt;
&lt;p&gt;To start this, you need at least two machines in a network setting that can access each other. You also should ensure that both Python environments have ray and other packages required by your code. One of the machines will serve as the master (head), and other machines (workers) will connect to it.&lt;/p&gt;
&lt;p&gt;First, you need to run ray on the head node using &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;​​ray start --head
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;You may face a question asking whether you want to share usage statistics that you can answer yes or no.&lt;/p&gt;
&lt;p&gt;The response should look like this:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;--------------------
Ray runtime started.
--------------------

Next steps
  To connect to this Ray runtime from another node, run
    ray start --address&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;192.168.1.2:6379&amp;#39;&lt;/span&gt;

  Alternatively, use the following Python code:
    import ray
    ray.init&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="nv"&gt;address&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;auto&amp;#39;&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;

  To connect to this Ray runtime from outside of the cluster, &lt;span class="k"&gt;for&lt;/span&gt; example to
  connect to a remote cluster from your laptop directly, use the following
  Python code:
    import ray
    ray.init&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="nv"&gt;address&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;ray://&amp;lt;head_node_ip_address&amp;gt;:10001&amp;#39;&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;

  If connection fails, check your firewall settings and network configuration.

  To terminate the Ray runtime, run
    ray stop
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;As the output suggests, you now need to connect workers to the head using&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;ray start --address&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;{HEAD_NODE_IP}:6379&amp;#39;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Remember to change the IP address with the IP of the head node.&lt;/p&gt;
&lt;p&gt;The response will be like:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;--------------------
Ray runtime started.
--------------------
To terminate the Ray runtime, run
  ray stop
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Now, to run your Python code, you need to do&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;ray job submit --address&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;{HEAD_NODE_IP}:6379&amp;#39;&lt;/span&gt; --working-dir . -- python main.py
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The working dir line is critical, specifically if your code uses other local files and packages.
The output would be like this:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;Job submission server address: http://127.0.0.1:8265
&lt;span class="m"&gt;2023&lt;/span&gt;-05-09 &lt;span class="m"&gt;19&lt;/span&gt;:13:16,062 INFO dashboard_sdk.py:360 -- Package gcs://_ray_pkg_6c68afec082a1408.zip already exists, skipping upload.
-------------------------------------------------------
Job &lt;span class="s1"&gt;&amp;#39;raysubmit_BAUvdD7m5EWcYXix&amp;#39;&lt;/span&gt; submitted successfully
-------------------------------------------------------
Next steps
  Query the logs of the job:
    ray job logs raysubmit_BAUvdD7m5EWcYXix
  Query the status of the job:
    ray job status raysubmit_BAUvdD7m5EWcYXix
  Request the job to be stopped:
    ray job stop raysubmit_BAUvdD7m5EWcYXix
Tailing logs &lt;span class="k"&gt;until&lt;/span&gt; the job exits &lt;span class="o"&gt;(&lt;/span&gt;disable with --no-wait&lt;span class="o"&gt;)&lt;/span&gt;:
&lt;span class="m"&gt;2023&lt;/span&gt;-05-09 &lt;span class="m"&gt;19&lt;/span&gt;:13:18,128 INFO worker.py:1230 -- Using address &lt;span class="m"&gt;10&lt;/span&gt;.155.105.194:6379 &lt;span class="nb"&gt;set&lt;/span&gt; &lt;span class="k"&gt;in&lt;/span&gt; the environment variable RAY_ADDRESS
&lt;span class="m"&gt;2023&lt;/span&gt;-05-09 &lt;span class="m"&gt;19&lt;/span&gt;:13:18,128 INFO worker.py:1342 -- Connecting to existing Ray cluster at address: &lt;span class="m"&gt;192&lt;/span&gt;.168.1.2:6379...
&lt;span class="m"&gt;2023&lt;/span&gt;-05-09 &lt;span class="m"&gt;19&lt;/span&gt;:13:18,136 INFO worker.py:1519 -- Connected to Ray cluster. View the dashboard at &lt;span class="m"&gt;127&lt;/span&gt;.0.0.1:8265 
Runs using multiple processes took &lt;span class="m"&gt;9&lt;/span&gt;.233 seconds
------------------------------------------
Job &lt;span class="s1"&gt;&amp;#39;raysubmit_BAUvdD7m5EWcYXix&amp;#39;&lt;/span&gt; succeeded
------------------------------------------
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;It now takes only 11 seconds, and I’m running on 76 CPU cores.&lt;/p&gt;
&lt;p&gt;If you have installed the ray with the default option, you can see the dashboard at the head ip address on port &lt;code&gt;8265&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Ray Dashboard" src="https://erfanloghmani.github.io/images/from-frustration-to-fast-how-to-ray/ray-dashboard.png"&gt;&lt;/p&gt;
&lt;p&gt;As suggested in the output of the submit command, you can see the logs and status of the job using its Submission_ID. You can also stop the job. The commands are&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;ray job logs &lt;span class="o"&gt;{&lt;/span&gt;SUBMISSION_ID&lt;span class="o"&gt;}&lt;/span&gt;
ray job status &lt;span class="o"&gt;{&lt;/span&gt;SUBMISSION_ID&lt;span class="o"&gt;}&lt;/span&gt;
ray job stop &lt;span class="o"&gt;{&lt;/span&gt;SUBMISSION_ID&lt;span class="o"&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;hr&gt;
&lt;p&gt;Wow, that was a lot of stuff to cover! In this post, we've explored how to distribute computation jobs using Ray for faster and more efficient computation. Looking at an example of SGD, we’ve seen how we can significantly reduce the running time of our computations by at least a factor of 10 (this can vary depending on the computation resources you have and your tasks).&lt;/p&gt;
&lt;p&gt;&lt;img alt="A generated image of a person working with a new and fast computer, Image credit: Microsoft/Bing" src="https://erfanloghmani.github.io/images/from-frustration-to-fast-how-to-ray/person-working-with-fast-computer.jpeg"&gt;&lt;/p&gt;
&lt;p&gt;Before we go, I want to share some additional tips and insights I've gained through my experience with Ray. I'll also provide some visualizations of the SGD experiment that you might find interesting.&lt;/p&gt;
&lt;h2&gt;Additional tips&lt;/h2&gt;
&lt;h3&gt;Port is not available&lt;/h3&gt;
&lt;p&gt;If you are getting this message&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;ConnectionError: Ray is trying to start at &lt;span class="m"&gt;192&lt;/span&gt;.168.1.2:6379, but is already running at &lt;span class="m"&gt;192&lt;/span&gt;.168.1.2:6379
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;when running &lt;code&gt;ray start --head&lt;/code&gt;, this could be because the port is unavailable or another user is running a ray cluster. You can simply specify another port using&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;ray start --head --port &lt;span class="o"&gt;{&lt;/span&gt;PORT_NUMBER&lt;span class="o"&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h3&gt;PermissionError: [Errno 13]&lt;/h3&gt;
&lt;p&gt;You may also get&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;Permission denied: &lt;span class="s1"&gt;&amp;#39;/tmp/ray/ray_current_cluster&amp;#39;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;when running the &lt;code&gt;start&lt;/code&gt; command. This error is probably because another user is running a cluster, and Ray wants to use the same temp directory but does not have the write access. To fix this, you can create another temp directory and give the address using&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;ray start –head --temp-dir &lt;span class="o"&gt;{&lt;/span&gt;PATH_TO_DIR&lt;span class="o"&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h3&gt;File outputs&lt;/h3&gt;
&lt;p&gt;By default, if you use a relative path for your file outputs, Ray would write them in the tmp directory created for the run on each machine. If you want to save files, the best way is to mount shared cloud storage to both machines, but if you are not comfortable with that, you can just use absolute paths and collect the files from each machine.&lt;/p&gt;
&lt;h3&gt;Dashboard behind firewall&lt;/h3&gt;
&lt;p&gt;Sometimes server machines don’t expose all the ports to the outside world – where your computer may be. If you have ssh access, you can simply do a port forwarding using the &lt;code&gt;-L&lt;/code&gt; option&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;ssh -L &lt;span class="m"&gt;8265&lt;/span&gt;:localhost:8265 user@server.domain.com
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Now you can access the dashboard from your browser at 
&lt;a href="http://localhost:8265"&gt;localhost:8265&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;SGD Results&lt;/h2&gt;
&lt;p&gt;Finally, let's look at some of the results obtained from the SGD algorithm. The figure below provides insights into the learning procedure over time. On the left plot, we observe the squared loss at each step, representing the square of the distance between the true y value and the prediction. The right plot illustrates the \( l_2 \) distance between the estimated and true beta values. Notably, both plots exhibit a rapid initial decay, aligning with the expected \( O(1/t) + O(lr) \) convergence rates.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Step loss and estimation error over time plots" src="https://erfanloghmani.github.io/images/from-frustration-to-fast-how-to-ray/estimation-plots.png"&gt;&lt;/p&gt;
&lt;p&gt;Moving on, the following figure showcases an animation presenting the histogram of the estimated betas along different repetitions of the algorithm through time. At each time step, the points collectively form a distribution resembling a normal distribution, gradually converging towards the true estimate by the end of the process. These visual representations provide valuable insights into the behavior and convergence of the SGD algorithm and demonstrate its effectiveness in approximating the true values.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Animation of histogram of estimation points throught time" src="https://erfanloghmani.github.io/images/from-frustration-to-fast-how-to-ray/histogram.gif"&gt;&lt;/p&gt;</content><category term="Blog"></category><category term="Coding"></category><category term="Academia"></category><category term="Blog"></category></entry><entry><title>Let's be more human than a chatbot</title><link href="https://erfanloghmani.github.io/lets-be-more-human.html" rel="alternate"></link><published>2023-04-24T10:08:00-07:00</published><updated>2023-04-24T10:08:00-07:00</updated><author><name>Erfan Loghmani</name></author><id>tag:erfanloghmani.github.io,2023-04-24:/lets-be-more-human.html</id><summary type="html">&lt;p&gt;I want to start this blog post by encouraging you to read this Reddit post.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;This made me shed a tear&lt;/strong&gt;&lt;br&gt;
ME: tell me something nice before i go to sleep, nobody spoke to me or even told me a good night&lt;br&gt;
CHATGPT: You are a unique and valuable person …&lt;/p&gt;&lt;/blockquote&gt;</summary><content type="html">&lt;p&gt;I want to start this blog post by encouraging you to read this Reddit post.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;This made me shed a tear&lt;/strong&gt;&lt;br&gt;
ME: tell me something nice before i go to sleep, nobody spoke to me or even told me a good night&lt;br&gt;
CHATGPT: You are a unique and valuable person with a special purpose in this world. You have the ability to make a positive impact on the lives of those around you, and your presence in this world is truly a gift. As you rest and rejuvenate tonight, know that you are loved and appreciated, and that tomorrow is a new day filled with endless possibilities. Good night!&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Isn’t it fascinating? A robot can be much empathic to make a person cry. At the same time, when I think about the post, it is a bit weird that listening to these nice words from a robot that you know is not a real person and does not know you can help.&lt;/p&gt;
&lt;h2&gt;Scrapping Reddit Mental Health Communities&lt;/h2&gt;
&lt;p&gt;Last week, I had the pleasure of diving into the world of Reddit API to satisfy my curiosity. I wanted to know how users in mental health communities responded to the release and availability of ChatGPT. I used the list of mental health related subreddits from Sharma, Choudhury 2018 paper, &lt;a href="https://dl.acm.org/doi/pdf/10.1145/3173574.3174215"&gt;Mental health support and its relationship to linguistic accommodation in online communities&lt;/a&gt;. Doing so, I collected 116 posts mentioning ChatGPT; 116 posts mentioning chatgpt in mental health communities since its release! I knew there should be some mentions, but the number was more than I expected. The top three subreddits with the most posts were OffMyChest, Lonely, and SocialAnxiety. But ChatGPT was also mentioned in a variety of other subreddits like Schizophrenia, Anxiety, and Depression.&lt;/p&gt;
&lt;p&gt;&lt;img alt="List of top 10 subreddits with most posts mentioning ChatGPT" src="https://erfanloghmani.github.io/images/lets-be-more-human/top-subreddits.png"&gt;&lt;/p&gt;
&lt;p&gt;To better understand the time trend, I created a histogram of the posts and found that mentions of ChatGPT started in late 2022, with an upward trend that indicates more attention to this amazing tool in mental health related communities on Reddit.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Histogram of post created time, showing an upward trend" src="https://erfanloghmani.github.io/images/lets-be-more-human/time-trend.png"&gt;&lt;/p&gt;
&lt;p&gt;The subreddit with the most mentions of chatgpt, offmychest, is a general community that people can talk about all kinds of emotional things. So I decided to first focus on the lonely subreddit that has a specific aim on loneliness. &lt;/p&gt;
&lt;h3&gt;Loneliness&lt;/h3&gt;
&lt;p&gt;Some of the posts that I came across in the lonelyness community mentioned that chatgpt is a good company for them and is both a good listener and a good source of advice — which can be concerning. Let’s look at some examples:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;I use ChatGpt to have at least something to chat with&lt;/strong&gt;&lt;br&gt;
I [M27] have no one to talk to sometimes i get so lonely i just open chatgpt and start talking with about life and stuff. &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;ChatGPT&lt;/strong&gt;&lt;br&gt;
GPT is nice company. It doesn't judge and gives the best advice it can. Though it is a AI it tries harder than most people do to reply. It's unfortunate that chatGPT can't be a real person. Who would have genuine emotion and opinions but will still have that neat AI feel to them. I think that would be a pretty good friend. Someone who can write well thought out paragraphs in order to answer your simple questions."&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Also, if you have seen the movie "Her," you may also have thought about how close we are to having the robots like in the movie. Same as what this post asks&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Is the AI from the movie "Her" possible with ChatGPT or Google's AI?&lt;/strong&gt;&lt;br&gt;
I recently &lt;a href="https://blog.acer.com/en/discussion/431/best-online-tools-to-make-you-less-lonely#latest"&gt;read an article&lt;/a&gt; on some apps to help deal with loneliness and most of them are either to keep you occupied or to have someone talk to you. Reading about &lt;a href="https://www.zdnet.com/article/what-is-chatgpt-and-why-does-it-matter-heres-everything-you-need-to-know/"&gt;ChatGPT&lt;/a&gt; and how it can mimic human speech, I was wondering how close we are to being able to replicate the AI from the movie "&lt;a href="https://en.wikipedia.org/wiki/Her_(film)"&gt;Her&lt;/a&gt;" and why I am so excited for it. (...)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;Final thoughts&lt;/h2&gt;
&lt;p&gt;Reading these posts, I think of myself and whether I have treated people around me well, listened to them, and was thoughtful of them, or whether a robot was better than me and more human. I know that, as humans, we have many limiting factors like time and space. Sometimes we should focus on our own goals and desires, and sometimes we are so far from a friend that we can't give them a hug when they most need it. While we have these limitations, it is good that often we can think of people around us and see if we are thoughtful, kind, and hearing to them. &lt;/p&gt;
&lt;p&gt;&lt;img alt="A robot empathically hugs a lonely person. This image was created with the assistance of DALL·E 2" src="https://erfanloghmani.github.io/images/lets-be-more-human/lonely-person-and-robot.png"&gt;&lt;/p&gt;
&lt;p&gt;As a final note, I want to show one last post. Reading through this one, it seems that the writer is in a critical situation where the chatbot may not be a good help, and brings up the importance of safety guards in new technology.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Maybe this will be forgotten and lost among all else.&lt;/strong&gt;&lt;br&gt;
I have already attempted 4 times and failed like many other things in life.&lt;br&gt;
It hurts to breathe.&lt;br&gt;
It hurts more to talk to others.&lt;br&gt;
I need some sort of AI. &lt;br&gt;
Not Replika, it is useless and feels no different than talking to people that I don't want to talk to.&lt;br&gt;
Something similar to chatgpt.&lt;br&gt;
I don't know how much longer I can keep going.&lt;br&gt;
(I am trying my best to draw the least attention, just want a good AI and I am out)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3&gt;P.S.&lt;/h3&gt;
&lt;p&gt;I've made the code I used to gather the data available on GitHub, which you can access &lt;a href="https://github.com/erfanloghmani/mental-health-reddit-crawl"&gt;here&lt;/a&gt;. While testing the code before pushing it, I made an exciting observation: it found 122 posts instead of 116, six more posts just in a few days.&lt;/p&gt;
&lt;p&gt;For those interested in exploring the data further, I've compiled the posts from two subreddits into this &lt;a href="https://docs.google.com/spreadsheets/d/1b9lfcl62V1cFv3PaQX-gujDaO-TrWMq4KvhiDhJZngQ/edit?usp=sharing"&gt;sheet&lt;/a&gt;. The complete data could be found in the repo. Happy digging!&lt;/p&gt;</content><category term="Blog"></category><category term="Life"></category><category term="Blog"></category></entry><entry><title>Being Frequentist? A Friendship Case</title><link href="https://erfanloghmani.github.io/being-frequentist.html" rel="alternate"></link><published>2020-11-02T23:12:00-08:00</published><updated>2020-11-02T23:12:00-08:00</updated><author><name>Erfan Loghmani</name></author><id>tag:erfanloghmani.github.io,2020-11-02:/being-frequentist.html</id><summary type="html">&lt;p&gt;You text a friend that you haven't seen for a long time. She/He replies, but not in a friendly manner, maybe just replying short answers like "I'm fine," "It's OK,"... After a week, you try again with the hope that you could schedule a phone/video call. She/He …&lt;/p&gt;</summary><content type="html">&lt;p&gt;You text a friend that you haven't seen for a long time. She/He replies, but not in a friendly manner, maybe just replying short answers like "I'm fine," "It's OK,"... After a week, you try again with the hope that you could schedule a phone/video call. She/He says it's a good idea and will inform you whenever she/he had time. Days pass, and nothing happens. I've recently been in these situations a few times, and maybe sometimes I was on the other side of the table. The question that comes up is how much you should continue this process, and when you should lose your hope. Or on the other side, how much should you expect your friends to give you time.&lt;/p&gt;
&lt;p&gt;You may have been in these situations as well and may have solutions and techniques for yourself. When I was thinking about this problem weeks ago, I reminded two statistical inference approaches and tried to formalize the problem with that insight. Let's first take a look at the concepts.&lt;/p&gt;
&lt;h2&gt;Frequentist vs Bayesian&lt;/h2&gt;
&lt;p&gt;There are two main approaches in statistical inference, frequentist and bayesian. I will consider the parameter estimation task in this article and compare these two approaches in this task. The parameter estimation task rises in situations like estimating the average height of men in society or estimating the probability of getting heads when flipping a strange coin.&lt;/p&gt;
&lt;p&gt;The frequentist approach only uses data and happenings of the past for its estimation. For instance, if the frequentist approach wants to estimate the probability of heads for a coin, it will use the previous results and counts the number of heads divided by the total number of trials. On the other hand, the Bayesian approach starts with an initial belief and updates it based on observations. To better explain these two, we will see the mathematics behind them for the coin flip problem.&lt;/p&gt;
&lt;h2&gt;Maths for coin flip problem&lt;/h2&gt;
&lt;h3&gt;Frequentist's view&lt;/h3&gt;
&lt;p&gt;Consider we have a coin and have flipped it three times. The first two outcomes were heads, and the last one was tails. The frequentist approach starts without anything in mind because we don't have any prior experiences with the coin. After the first toss, as the outcome is heads, it estimates the probability to be \( \frac{1}{1} \). By the next two tosses, it will update the estimates to \( \frac{2}{2} \) and \( \frac{2}{3} \) consequently.&lt;/p&gt;
&lt;h3&gt;Bayesian's view&lt;/h3&gt;
&lt;p&gt;Analyzing the Bayesian approach is not that straightforward. First, we have to set the initial belief. For simplicity, I will consider the initial belief to have equal weights for these five potential values: \( \{0, \frac{1}{4}, \frac{2}{4}, \frac{3}{4}, 1\} \). We will consider the continuous initial belief in advance. The initial belief will be like:&lt;/p&gt;
&lt;p&gt;&lt;img alt="Initial belief" src="https://erfanloghmani.github.io/images/being-freq/dist-1.png"&gt;&lt;/p&gt;
&lt;p&gt;The Bayesian approach uses the Bayes formula to update its beliefs after each observation. That is&lt;/p&gt;
&lt;p&gt;&lt;img alt="Bayes Theorem" src="https://erfanloghmani.github.io/images/being-freq/1-bayes.png"&gt;&lt;/p&gt;
&lt;p&gt;In this example, \( \theta \) is any of the five potential probability values, and \( D \) is the observations. Let's compute the denominator for the first trial&lt;/p&gt;
&lt;p&gt;&lt;img alt="Probability of getting heads on first trial" src="https://erfanloghmani.github.io/images/being-freq/2-d1-head.png"&gt;&lt;/p&gt;
&lt;p&gt;This means before the first toss; we expect the outcome's probability of being heads to be \( \frac{1}{2} \).
Now let's compute the new probabilities of parameters after the first toss.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Updated probabilities after first trial" src="https://erfanloghmani.github.io/images/being-freq/3-update-after-d1.png"&gt;&lt;/p&gt;
&lt;p&gt;We see that after the first observation, the Bayesian approach stops putting any weight on the \( \theta = 0 \) condition, as it has seen one head so far. But unlike the frequentist approach, it doesn't immediately says that the probability of getting heads is one and still puts some weights on other possibilities. The calculation continues as&lt;/p&gt;
&lt;p&gt;&lt;img alt="Probability of getting heads on second trial" src="https://erfanloghmani.github.io/images/being-freq/4-d2-head.png"&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="Updated probabilities after second trial" src="https://erfanloghmani.github.io/images/being-freq/5-update-after-d2.png"&gt;&lt;/p&gt;
&lt;p&gt;And finally&lt;/p&gt;
&lt;p&gt;&lt;img alt="Probability of getting tails on third trial" src="https://erfanloghmani.github.io/images/being-freq/6-d3-tail.png"&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="Updated probabilities after third trial" src="https://erfanloghmani.github.io/images/being-freq/7-update-after-d3.png"&gt;&lt;/p&gt;
&lt;p&gt;The Bayesian approach started with a uniform belief and updated it to give more weights on probabilities near one after seeing the first two heads. Following the third observation, it also removes the weight from \( \theta = 1 \) and skews the probabilities toward lower values.&lt;/p&gt;
&lt;h3&gt;Comparison&lt;/h3&gt;
&lt;p&gt;The frequentist approach gives a unique value for the estimated probability, while the Bayesian approach holds a probability distribution over the values. To reach one estimation value for the Bayesian method, we could take the expectation of the distribution. This way, the estimated probability of getting head in these two approaches will be:&lt;/p&gt;
&lt;p&gt;&lt;img alt="Estimated probabilities for each approach" src="https://erfanloghmani.github.io/images/being-freq/8-table.png"&gt;&lt;/p&gt;
&lt;h3&gt;Continuous version&lt;/h3&gt;
&lt;p&gt;While I had simplified the above example to a discrete version, you can play with the continuous version of the bayesian approach starting from uniform distribution in the section below.&lt;/p&gt;
&lt;iframe id="prior" width="100%" height="480" src="bayesian-inference/index.html"&gt;&lt;/iframe&gt;

&lt;p&gt;I have taken the codes for this part from the &lt;a href="https://seeing-theory.brown.edu/bayesian-inference/index.html"&gt;Seeing Theory&lt;/a&gt; website. You can check it out and find other exciting visualizations for probability and statistics concepts.&lt;/p&gt;
&lt;p&gt;In the continuous version, &lt;a href="http://www.cs.tau.ac.il/~mansour/ml-course-12/Scribe2_Bayes.pdf"&gt;it is shown that&lt;/a&gt; the expectation of the Bayesian estimate is \( \frac{H + 1}{N + 2} \), where \( H \) is the number of heads and \( N \) is the total number of trials. On the other hand, the frequentist approach estimates the parameter to be \( \frac{H}{N} \). It is somehow like that the Bayesian method adds one head and one tail to the beginning of the sequence.&lt;/p&gt;
&lt;h2&gt;Back to the friendship case&lt;/h2&gt;
&lt;p&gt;The friendship scenarios are related to the coin flip example as I always have to estimate my friend's probability of responding appropriately. Then, maybe if I see the estimated probability is less than a threshold, I will lose my hope and don't text her/him the next time.&lt;/p&gt;
&lt;p&gt;Let's define an optimistic line that indicates how hopeful you are about your friend's next behavior.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Optimistic line" src="https://erfanloghmani.github.io/images/being-freq/9-opt-line.png"&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style="font-size:0.9em;"&gt;&lt;strong&gt;Notice:&lt;/strong&gt; The faces are designed by Bonita90 from &lt;a href="https://99designs.co.uk/buttons-icons/contests/button-icon-cancerlife-105934"&gt;99design&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;We want to see where each statistical approach will stand on this line in different situations.&lt;/p&gt;
&lt;p&gt;First, consider a situation where you had two unsuccessful attempts in connecting with your friend. The frequentist approach estimates the probability as \( \frac{0}{2} \) while the Bayesian approach's average estimate will be \( \frac{1}{4} \). So the line will be&lt;/p&gt;
&lt;p&gt;&lt;img alt="Optimistic line after two bad happenings" src="https://erfanloghmani.github.io/images/being-freq/10-opt-line-bad.png"&gt;&lt;/p&gt;
&lt;p&gt;Oppositely, consider a situation where both of your first attempts were successful. Now the frequentist approach will estimate \( \frac{2}{2} \), and the Bayesian approach will estimate \( \frac{3}{4} \) on average. And we get&lt;/p&gt;
&lt;p&gt;&lt;img alt="Optimistic line after two good happenings" src="https://erfanloghmani.github.io/images/being-freq/11-opt-line-good.png"&gt;&lt;/p&gt;
&lt;p&gt;What about you? Where do you stand on the optimistic line in each of these scenarios? Of course, the answer may differ based on other factors, the details of what has happened between you, or maybe your personality and how much you like that friend. When I discussed this issue with some of my friends, some of them told me they are more optimistic in most cases because they know that everyone could face hard times and we have to be more patient in friendships.&lt;/p&gt;
&lt;p&gt;While you are free to decide where you want to stand, personally, I think the frequentist approach is not a good option. I have some problems with this approach in both examples. It seems to me that the frequentist approach makes a harsh decision too early that may have harmful consequences. In the first example, it may lose hope early, causing a valuable friendship to die. On the other side, being optimistic based on a few good experiences may higher the expectations, and future bad outcomes may hurt you more. Overall, I think taking a moment before each decision and seeing it from this perspective is helpful. We could check how much evidence we have for our decisions and how confident we should be based on them.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Let me know your thoughts on this issue. Also, it will be great if you mention what other things we should consider in our modeling.&lt;/strong&gt;&lt;/p&gt;
&lt;h2&gt;Thanks&lt;/h2&gt;
&lt;p&gt;Thank you, Matin Ansaripour, Bahar Salamatian, and Atia Hamidizadeh, for providing your time to discuss this topic, which helped me see the subject from new perspectives and organize this blog post. Additionally, Sina Rismanchian, Tannaz Azari, and again Bahar Salamatian to proofread the article and providing their opinions and excellent suggestions.&lt;/p&gt;</content><category term="Blog"></category><category term="Life"></category><category term="Blog"></category></entry><entry><title>M.Sc. seminar</title><link href="https://erfanloghmani.github.io/msc-seminar.html" rel="alternate"></link><published>2020-01-30T20:34:00-08:00</published><updated>2020-01-30T20:34:00-08:00</updated><author><name>Erfan Loghmani</name></author><id>tag:erfanloghmani.github.io,2020-01-30:/msc-seminar.html</id><summary type="html">&lt;h1&gt;Representation Learning on Dynamic Graphs&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;Abstract:&lt;/strong&gt; Graphs are a common language in modeling several problems, from social and economic networks to interactions in cells and brain neurons.
According to the availability of an enormous amount of data from graphs, Machine Learning algorithms gained lots of attention in this area. But …&lt;/p&gt;</summary><content type="html">&lt;h1&gt;Representation Learning on Dynamic Graphs&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;Abstract:&lt;/strong&gt; Graphs are a common language in modeling several problems, from social and economic networks to interactions in cells and brain neurons.
According to the availability of an enormous amount of data from graphs, Machine Learning algorithms gained lots of attention in this area. But the main challenge is how to represent and encode nodes so that such models could interpret the data and the representation preserve the main features of nodes.&lt;/p&gt;
&lt;p&gt;There are different types of methods for this problem, some are based on a distance function defined on nodes, some are based on random walks like DeepWalk and node2vec, there is also a range of methods based on Deep Learning models like Graph Convolutional Networks and GraphSAGE.
These methods are originally designed for static graphs and learn representations for one snapshot of a graph, while in most problems graphs evolve in time and new nodes and edges arrive and leave and the model has to encounter these events and update representations meanwhile.&lt;/p&gt;
&lt;p&gt;In this research, we are going to first examine the latest methods proposed for dynamic graph representation learning and then try to enhance them in tasks like link prediction and node classification.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://erfanloghmani.github.io/files/MSc-seminar-presentation.pdf" target="_blank"&gt;Presentation file&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;embed src="https://drive.google.com/viewerng/
viewer?embedded=true&amp;url=https://erfanloghmani.github.io/files/MSc-seminar-presentation.pdf" width="100%" height="450"&gt;&lt;/p&gt;</content><category term="Academia"></category><category term="Presentation"></category><category term="Academia"></category></entry><entry><title>ICTP workshop</title><link href="https://erfanloghmani.github.io/ictp-workshop.html" rel="alternate"></link><published>2019-10-12T20:21:00-07:00</published><updated>2019-10-12T20:21:00-07:00</updated><author><name>Erfan Loghmani</name></author><id>tag:erfanloghmani.github.io,2019-10-12:/ictp-workshop.html</id><summary type="html">&lt;p&gt;Last week I had the chance to attend the &lt;a href="http://indico.ictp.it/event/8722/overview"&gt;ICTP workshop on science of data science&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;From the very beginning of landing at Venice to the last moments of my visit, there was lots of learning for me. Not just from the lectures, but from all the people I've met …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Last week I had the chance to attend the &lt;a href="http://indico.ictp.it/event/8722/overview"&gt;ICTP workshop on science of data science&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;From the very beginning of landing at Venice to the last moments of my visit, there was lots of learning for me. Not just from the lectures, but from all the people I've met, and maybe some from myself :)
Thank you, organizers, to bring all these people together and also for your kindness &amp;amp; hospitality.&lt;/p&gt;
&lt;p&gt;&lt;img alt="ICTP workshop on science of data science group photo" src="http://indico.ictp.it/event/8722/material/2/0.jpg"&gt;&lt;/p&gt;
&lt;p&gt;I also had a great experience presenting an intoductory poster of my research problem and ongoing works in the poster session.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://erfanloghmani.github.io/files/ICTP_Poster.pdf" target="_blank"&gt;My poster&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;embed src="https://drive.google.com/viewerng/
viewer?embedded=true&amp;url=https://erfanloghmani.github.io/files/ICTP_Poster.pdf" width="100%" height="800"&gt;&lt;/p&gt;</content><category term="Academia"></category><category term="Poster"></category><category term="Academia"></category><category term="Travel"></category></entry><entry><title>Don't Repeat Yourself</title><link href="https://erfanloghmani.github.io/dont-repeat-yourself.html" rel="alternate"></link><published>2019-09-06T18:17:00-07:00</published><updated>2019-09-06T18:17:00-07:00</updated><author><name>Erfan Loghmani</name></author><id>tag:erfanloghmani.github.io,2019-09-06:/dont-repeat-yourself.html</id><summary type="html">&lt;p&gt;You may have heard about the DRY principle in coding &amp;amp; software design. It says that by using function definitions and OOP concepts you can avoid repeating the code. But I'm not going to write about the DRY principle this way; I found it somewhere else in my life.&lt;/p&gt;
&lt;p&gt;Last week …&lt;/p&gt;</summary><content type="html">&lt;p&gt;You may have heard about the DRY principle in coding &amp;amp; software design. It says that by using function definitions and OOP concepts you can avoid repeating the code. But I'm not going to write about the DRY principle this way; I found it somewhere else in my life.&lt;/p&gt;
&lt;p&gt;Last week I was a mentor in &lt;a href="http://summerschool.ce.sharif.edu/register"&gt;CSSS2019&lt;/a&gt; which stands for Computer Science Summer School 2019. The school was aimed to gather high school students and show them some inspiring and insightful topics in CS. In the two weeks of the school, I facilitated the same group discussion six times, but to six different groups of students. I was one of the mentors of the probability workshop and we wanted to start the class with a group discussion about the meaning of probability. After the fourth class, I suddenly saw a pattern. The discussions in the first class of each day went far better than in the second class. So I tried to find the reason and here's what I found: When the first class went well, I just wanted to repeat them in the second class. So the examples were not original and didn't work, I didn't respond well to students and make them participate. And the most important thing I found is I forgot to love the students, not the topic. With the experience of the first class of the day and the good feedback I got, maybe my mind increased the value of the topic and caused me to forget loving the students. After seeing these facts, I think the two remaining classes went extremely better and I had a better feeling at the end of both classes.&lt;/p&gt;
&lt;p&gt;This situation happens frequently, when we do something well once, we try to repeat that in the same way next time, but it doesn't work like that. For example, if once I felt sad and after going out with friends I felt better, the next time that I'm sad if I just try to repeat that and schedule a hangout it may not work. Because this time I missed the original goal of hangouts to see friends and I'm just using it to solve my problem.&lt;/p&gt;
&lt;p&gt;I'm trying this myself, and hope you can too.&lt;/p&gt;
&lt;p&gt;&lt;img alt="CSSS 2019 Tehran" src="https://erfanloghmani.github.io/images/csss2019-all.jpg"&gt;&lt;/p&gt;</content><category term="Blog"></category><category term="Blog"></category><category term="Life"></category></entry><entry><title>Start</title><link href="https://erfanloghmani.github.io/start.html" rel="alternate"></link><published>2019-06-29T23:43:00-07:00</published><updated>2019-06-29T23:43:00-07:00</updated><author><name>Erfan Loghmani</name></author><id>tag:erfanloghmani.github.io,2019-06-29:/start.html</id><content type="html">&lt;p&gt;Everything has an start!&lt;/p&gt;</content><category term="Blog"></category><category term="Blog"></category></entry><entry><title>My B.Sc. project</title><link href="https://erfanloghmani.github.io/bsc-project.html" rel="alternate"></link><published>2018-07-03T18:17:00-07:00</published><updated>2018-07-03T18:17:00-07:00</updated><author><name>Erfan Loghmani</name></author><id>tag:erfanloghmani.github.io,2018-07-03:/bsc-project.html</id><summary type="html">&lt;h1&gt;Profiling Researchers Based on Features Extracted from Articles and Citations&lt;/h1&gt;
&lt;p&gt;A researcher may be evaluated based on several measures such as the total number of published papers, number of citations,  and her/his h-index. These measures may not reflect the real quality of researchers as many self citations and citing …&lt;/p&gt;</summary><content type="html">&lt;h1&gt;Profiling Researchers Based on Features Extracted from Articles and Citations&lt;/h1&gt;
&lt;p&gt;A researcher may be evaluated based on several measures such as the total number of published papers, number of citations,  and her/his h-index. These measures may not reflect the real quality of researchers as many self citations and citing co-authors are observed during last decades. On the other hand, having a realistic quantitative measure for scientists has enormous impact on decisions taken in academia including hiring and  promoting faculty members. This research aims to develop a mathematical model for evaluating scientific researchers. The model uses authors' interactions and provides a quantitative value as a proxy of author’s quality. There are two existing models named R-model and Q-model for the same purpose. Our proposed model improves on these models by incorporating an iterative strategy to obtain best quantitate evolution of the researchers. Practically, we have tested our model  on DBLP dataset.&lt;/p&gt;
&lt;p&gt;I've done my B.Sc. project under &lt;a href="http://sharif.edu/~motahari/index.html"&gt;Dr. Motahari's&lt;/a&gt; supervision.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://erfanloghmani.github.io/files/profiling-researchers.pdf" target="_blank"&gt;Presentation file&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;embed src="https://drive.google.com/viewerng/
viewer?embedded=true&amp;url=https://erfanloghmani.github.io/files/profiling-researchers.pdf" width="100%" height="450"&gt;&lt;/p&gt;</content><category term="Academia"></category><category term="Presentation"></category><category term="Academia"></category></entry></feed>